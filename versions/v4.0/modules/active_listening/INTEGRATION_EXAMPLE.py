"""
Example of integrating Active Listening module into training_service.py

This shows how to replace existing active listening code with the module.
"""

# ==================== BEFORE (current code) ====================

async def process_question_OLD(self, user_id, question, scenario_config):
    """Old implementation with inline active listening logic."""
    session = self.user_service.get_session(user_id)

    # Old way: inline context check
    is_contextual = False
    last_resp = session.get('last_client_response', '')
    if last_resp:
        is_contextual = await self.question_analyzer.check_context_usage(
            question,
            last_resp,
            lambda kind, sys, usr: self.llm_service.call_llm('context', sys, usr),
            scenario_config.get('prompts', {})
        )

    # Old way: simple emoji badge
    context_badge = ""
    if is_contextual:
        session['contextual_questions'] += 1
        contextual_bonus = 5
        session['clarity_level'] += contextual_bonus
        context_badge = " ğŸ‘‚"  # âŒ Simple emoji only

    # Format response
    response = f"Ğ¢Ğ¸Ğ¿ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°: Ğ¡Ğ¸Ñ‚ÑƒĞ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹{context_badge}"
    # Result: "Ğ¢Ğ¸Ğ¿ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°: Ğ¡Ğ¸Ñ‚ÑƒĞ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ ğŸ‘‚"


# ==================== AFTER (with module) ====================

from modules.active_listening import ActiveListeningDetector, ActiveListeningConfig

# Initialize once (at bot startup)
active_listening_config = ActiveListeningConfig(
    enabled=True,
    use_llm=True,
    bonus_points=5,
    emoji="ğŸ‘‚",
    language="ru"
)
active_listening = ActiveListeningDetector(active_listening_config)


async def process_question_NEW(self, user_id, question, scenario_config):
    """New implementation with Active Listening module."""
    session = self.user_service.get_session(user_id)

    # âœ… New way: use module
    last_resp = session.get('last_client_response', '')

    is_contextual = await active_listening.check_context_usage(
        question=question,
        last_response=last_resp,
        call_llm_func=self.llm_service.call_llm
    )

    # âœ… New way: format badge with text
    context_badge = ""
    if is_contextual:
        session['contextual_questions'] += 1
        bonus = active_listening.get_bonus_points()
        session['clarity_level'] += bonus
        context_badge = active_listening.format_badge()  # âœ… Returns full badge

    # Format response
    response = f"Ğ¢Ğ¸Ğ¿ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°: Ğ¡Ğ¸Ñ‚ÑƒĞ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹{context_badge}"
    # Result: "Ğ¢Ğ¸Ğ¿ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°: Ğ¡Ğ¸Ñ‚ÑƒĞ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ ğŸ‘‚ (Ğ£ÑĞ¿ĞµÑˆĞ½Ğ¾Ğµ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ»ÑƒÑˆĞ°Ğ½Ğ¸Ğµ)"


# ==================== COMPARISON ====================

"""
BEFORE:
â”œâ”€ context_badge = " ğŸ‘‚"
â””â”€ Result: "Ğ¡Ğ¸Ñ‚ÑƒĞ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ ğŸ‘‚"

AFTER:
â”œâ”€ context_badge = active_listening.format_badge()
â””â”€ Result: "Ğ¡Ğ¸Ñ‚ÑƒĞ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ ğŸ‘‚ (Ğ£ÑĞ¿ĞµÑˆĞ½Ğ¾Ğµ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ»ÑƒÑˆĞ°Ğ½Ğ¸Ğµ)"

âœ… More obvious and informative!
"""


# ==================== FULL EXAMPLE ====================

async def handle_user_question_FULL_EXAMPLE(self, user_id: int, question: str):
    """Complete example with all steps."""
    session = self.user_service.get_session(user_id)
    scenario_config = self.scenario_loader.get_config()

    # 1. Classify question type
    question_type = self.question_analyzer.classify_question(
        question=question,
        question_types=scenario_config['question_types']
    )
    question_type_name = question_type.get('name', 'Ğ’Ğ¾Ğ¿Ñ€Ğ¾Ñ')

    # 2. Check active listening
    last_response = session.get('last_client_response', '')

    is_contextual = await active_listening.check_context_usage(
        question=question,
        last_response=last_response,
        call_llm_func=self.llm_service.call_llm
    )

    # 3. Format badge
    context_badge = ""
    if is_contextual:
        session['contextual_questions'] = session.get('contextual_questions', 0) + 1
        bonus_points = active_listening.get_bonus_points()
        session['clarity_level'] = min(100, session['clarity_level'] + bonus_points)
        context_badge = active_listening.format_badge()
        # Returns: " ğŸ‘‚ (Ğ£ÑĞ¿ĞµÑˆĞ½Ğ¾Ğµ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ»ÑƒÑˆĞ°Ğ½Ğ¸Ğµ)"

    # 4. Generate client response
    client_response = await self.generate_client_response(question)
    session['last_client_response'] = client_response

    # 5. Format feedback message
    feedback = f"""
ğŸ“ Ğ¢Ğ¸Ğ¿ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°: {question_type_name}{context_badge}

ğŸ’¬ ĞÑ‚Ğ²ĞµÑ‚ ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ğ°:
{client_response}

ğŸ“Š ĞŸÑ€Ğ¾Ğ³Ñ€ĞµÑÑ: {session['question_count']}/{scenario_config['game_rules']['max_questions']}
ğŸ¯ Ğ¯ÑĞ½Ğ¾ÑÑ‚ÑŒ: {session['clarity_level']}%
"""

    return feedback


# ==================== OUTPUT EXAMPLES ====================

"""
Example 1: Regular question (no active listening)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ“ Ğ¢Ğ¸Ğ¿ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°: Ğ¡Ğ¸Ñ‚ÑƒĞ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ

ğŸ’¬ ĞÑ‚Ğ²ĞµÑ‚ ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ğ°:
Ğ£ Ğ½Ğ°Ñ ĞºĞ¾Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° 50 Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞº.
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Example 2: With active listening
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ“ Ğ¢Ğ¸Ğ¿ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°: ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ½Ñ‹Ğ¹ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ ğŸ‘‚ (Ğ£ÑĞ¿ĞµÑˆĞ½Ğ¾Ğµ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ»ÑƒÑˆĞ°Ğ½Ğ¸Ğµ)

ğŸ’¬ ĞÑ‚Ğ²ĞµÑ‚ ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ğ°:
Ğ˜Ğ· ÑÑ‚Ğ¸Ñ… 50 Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞº Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ½Ğ¾ 15 Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ğ² Ğ¿Ñ€Ğ¾Ğ´Ğ°Ğ¶Ğ°Ñ….
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

âœ… Much more obvious that active listening was detected!
"""


# ==================== DISABLE MODULE ====================

# If you don't want active listening in a bot:
active_listening_config = ActiveListeningConfig(enabled=False)
active_listening = ActiveListeningDetector(active_listening_config)

# Now format_badge() will return empty string
context_badge = active_listening.format_badge()  # Returns: ""
